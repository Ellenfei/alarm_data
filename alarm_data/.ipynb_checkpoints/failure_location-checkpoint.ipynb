{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "file = \"单板信息报表_11-24-2017_11-48-17.xls\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 包的简单使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "光模块信息报表_11-24-2017_12-00-20.xls\n",
      "单板信息报表_11-24-2017_11-48-17.xls\n",
      "当前性能数据_11-24-2017_12-10-02.xls\n",
      "items is : ['光模块信息报表_11-24-2017_12-00-20.xls', '单板信息报表_11-24-2017_11-48-17.xls', '当前性能数据_11-24-2017_12-10-02.xls']\n"
     ]
    }
   ],
   "source": [
    "# sp的包\n",
    "# state, output = sp.getstatusoutput(cmd)\n",
    "state,output = sp.getstatusoutput(\"ls | grep '.xls'\")\n",
    "print(state)\n",
    "print(output)\n",
    "items = output.split(\"\\n\")\n",
    "print(\"items is :\", items)\n",
    "\n",
    "# pd\n",
    "# 1. read_csv/excel\n",
    "# 2. pd.DataFrame\n",
    "# 3. pd.Series\n",
    "dfs = []\n",
    "for item in items:\n",
    "    df = pd.read_excel(item, header=7)  # pd.DataFrame\n",
    "    dfs.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = dfs[0].groupby('光模块类型')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>光口</th>\n",
       "      <th>光模块类型</th>\n",
       "      <th>纤缆类型</th>\n",
       "      <th>逻辑类型</th>\n",
       "      <th>物理类型</th>\n",
       "      <th>序列号</th>\n",
       "      <th>CLEI码</th>\n",
       "      <th>厂商</th>\n",
       "      <th>生产日期</th>\n",
       "      <th>用户标识</th>\n",
       "      <th>备注</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOM编码</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34060473</th>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34060487</th>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           光口  光模块类型  纤缆类型  逻辑类型  物理类型  序列号  CLEI码   厂商  生产日期  用户标识   备注\n",
       "BOM编码                                                                   \n",
       "34060473   82     82    82    82    82   82     82   82    82    82   82\n",
       "34060487  123    123   123   123   123  123    123  123   123   123  123"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfs[0]\n",
    "df[(df['光模块类型']=='MXPD-243S')].groupby('BOM编码').count()\n",
    "a = grouped.count()\n",
    "a[a['光口']>10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch简单使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor v.s. Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch  # 总包\n",
    "import torch.nn as nn  # 神经网络包\n",
    "from torch.autograd import Variable as V  # 支持梯度下降操作的变量包\n",
    "import torchvision.transforms as transforms  # 辅助包\n",
    "import torch.optim as optim  # optimizer SDG, Adam, RMSprop\n",
    "\n",
    "# Tensor \n",
    "# Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensor vs Variable\n",
    "a = torch.randn(2, 2)  # list, np.ndarray\n",
    "b = torch.randn(2, 2)\n",
    "\n",
    "var_a = V(a, requires_grad=True)\n",
    "var_b = V(b)\n",
    "\n",
    "c = torch.add(var_a, var_b)\n",
    "d = torch.sum(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor不能求导，Variable可以求导\n",
    "d.backward()\n",
    "var_a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Variable containing:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Variable有两个成员，一个是grad，一个是grad_fn，是计算梯度\n",
    "m = V(torch.Tensor(3), requires_grad=True)\n",
    "print(m.grad)\n",
    "n = m.sum()\n",
    "n.backward()\n",
    "print(m.grad)\n",
    "print(m.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn\n",
    "1. Layers\n",
    "    1. nn.Linear\n",
    "    2. nn.Conv1/2/3d\n",
    "2. Activation\n",
    "    1. ReLU\n",
    "3. Normalize\n",
    "    1. Batch Normalization\n",
    "4. Loss\n",
    "    1. CrossEntropyLoss\n",
    "    2. L1Loss\n",
    "    3. L2Loss\n",
    "5. Optimizer\n",
    "    1. SGD\n",
    "    2. Adam\n",
    "    3. RMSProp\n",
    "6. Scheduler\n",
    "    1. ReducedOnPlateaus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### nn.Linear 全连接神经网络\n",
    "linear_1 = nn.Linear(in_features=9, out_features=3, bias=True) # input is 3x3\n",
    "# 查看网络参数\n",
    "print(linear_1.state_dict())\n",
    "for param in linear_1.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1\n",
      "[torch.FloatTensor of size 1x3x9x9]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.4911  0.4911  0.4911  0.4911  0.4911  0.4911  0.4911\n",
      "  0.4911  0.4911  0.4911  0.4911  0.4911  0.4911  0.4911\n",
      "  0.4911  0.4911  0.4911  0.4911  0.4911  0.4911  0.4911\n",
      "  0.4911  0.4911  0.4911  0.4911  0.4911  0.4911  0.4911\n",
      "  0.4911  0.4911  0.4911  0.4911  0.4911  0.4911  0.4911\n",
      "  0.4911  0.4911  0.4911  0.4911  0.4911  0.4911  0.4911\n",
      "  0.4911  0.4911  0.4911  0.4911  0.4911  0.4911  0.4911\n",
      "[torch.FloatTensor of size 1x1x7x7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## nn.Conv2d\n",
    "## stride, padding, margin, groups, dialation, mode(tf), kernel_size, filler, bias, in_channels, out_channels\n",
    "## nxnxc --> mxmx1\n",
    "conv1 = nn.Conv2d(3, 1, [3,3]) # in-1, out-3, ker-3x3\n",
    "inp = V(torch.ones(1, 3, 9, 9), requires_grad=False) # NCHW\n",
    "print(inp)\n",
    "print(conv1(inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_{out} = floor((H_{in}  + 2 * padding[0] - dilation[0] * (kernel\\_size[0] - 1) - 1) / stride[0] + 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.0899  0.4849 -0.6322\n",
      " -0.2600  1.0619  0.0617\n",
      " -0.9261 -1.4145  1.4038\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.2464  1.0086  1.2571\n",
      "  0.6852 -1.7824  0.5445\n",
      " -0.7537 -0.5808 -0.4045\n",
      "[torch.FloatTensor of size 2x3x3]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.0899  0.4849  0.0000\n",
      "  0.0000  1.0619  0.0617\n",
      "  0.0000  0.0000  1.4038\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.0000  1.0086  1.2571\n",
      "  0.6852  0.0000  0.5445\n",
      "  0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 2x3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# activation\n",
    "relu = nn.ReLU()\n",
    "tmp = V(torch.randn(2,3,3))\n",
    "print(tmp)\n",
    "print(relu(tmp))  # element-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BatchNorm2d in module torch.nn.modules.batchnorm:\n",
      "\n",
      "class BatchNorm2d(_BatchNorm)\n",
      " |  Applies Batch Normalization over a 4d input that is seen as a mini-batch\n",
      " |  of 3d inputs\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
      " |  \n",
      " |  The mean and standard-deviation are calculated per-dimension over\n",
      " |  the mini-batches and gamma and beta are learnable parameter vectors\n",
      " |  of size C (where C is the input size).\n",
      " |  \n",
      " |  During training, this layer keeps a running estimate of its computed mean\n",
      " |  and variance. The running sum is kept with a default momentum of 0.1.\n",
      " |  \n",
      " |  During evaluation, this running mean/variance is used for normalization.\n",
      " |  \n",
      " |  Because the BatchNorm is done over the `C` dimension, computing statistics\n",
      " |  on `(N, H, W)` slices, it's common terminology to call this Spatial BatchNorm\n",
      " |  \n",
      " |  Args:\n",
      " |      num_features: num_features from an expected input of\n",
      " |          size batch_size x num_features x height x width\n",
      " |      eps: a value added to the denominator for numerical stability.\n",
      " |          Default: 1e-5\n",
      " |      momentum: the value used for the running_mean and running_var\n",
      " |          computation. Default: 0.1\n",
      " |      affine: a boolean value that when set to ``True``, gives the layer learnable\n",
      " |          affine parameters. Default: ``True``\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(N, C, H, W)`\n",
      " |      - Output: :math:`(N, C, H, W)` (same shape as input)\n",
      " |  \n",
      " |  Examples:\n",
      " |      >>> # With Learnable Parameters\n",
      " |      >>> m = nn.BatchNorm2d(100)\n",
      " |      >>> # Without Learnable Parameters\n",
      " |      >>> m = nn.BatchNorm2d(100, affine=False)\n",
      " |      >>> input = autograd.Variable(torch.randn(20, 100, 35, 45))\n",
      " |      >>> output = m(input)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BatchNorm2d\n",
      " |      _BatchNorm\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods inherited from _BatchNorm:\n",
      " |  \n",
      " |  __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  forward(self, input)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overriden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  reset_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          parameter (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`torch-nn-init`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.data.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>>\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all parameters and buffers to double datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all parameters and buffers to half datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True`` then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :func:`state_dict()` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): A dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool): Strictly enforce that the keys in :attr:`state_dict`\n",
      " |              match the keys returned by this module's `:func:`state_dict()`\n",
      " |              function.\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None\n",
      " |      \n",
      " |      The hook should not modify the input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          parameter (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      When keep_vars is ``True``, it returns a Variable for each parameter\n",
      " |      (rather than a Tensor).\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional):\n",
      " |              if not None, the return dictionary is stored into destination.\n",
      " |              Default: None\n",
      " |          prefix (string, optional): Adds a prefix to the key (name) of every\n",
      " |              parameter and buffer in the result dictionary. Default: ''\n",
      " |          keep_vars (bool, optional): if ``True``, returns a Variable for each\n",
      " |              parameter. If ``False``, returns a Tensor for each parameter.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to dst_type.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Conv --> BN --> ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$loss(x, class) = -log(exp(x[class]) / (\\sum_j exp(x[j])))\n",
    "                     = -x[class] + log(\\sum_j exp(x[j]))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction is Variable containing:\n",
      "    1     1     1     1     1     1     1     1     1     1\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n",
      "target is  Variable containing:\n",
      " 3\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "cross entropy loss is  Variable containing:\n",
      " 2.3026\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "cross = nn.CrossEntropyLoss()\n",
    "l1 = nn.L1Loss\n",
    "pred = V(torch.ones(1,10))\n",
    "target = V(torch.LongTensor([3]))\n",
    "print(\"prediction is\", pred)\n",
    "print(\"target is \", target)\n",
    "# type(target)\n",
    "print(\"cross entropy loss is \", cross(pred, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = nn.Conv2d(3,3,3)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构件上很精网络\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        self.in_channels = in_channels\n",
    "        self.model = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels=in_channels, out_channels=5, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(in_channels=5, out_channels=3, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.AvgPool1d(kernel_size=2))\n",
    "        self.fc1 = self.Linear(3, 5)\n",
    "        self.fc2 = self.Linear(5, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # shape of x is (N, 1, 8)\n",
    "        # (N,1,8) --> (N, 5, 4)  --> (N, 3, 2)  --> (N, 3, 1)\n",
    "        x = self.model(x)\n",
    "        # (N, 3, 1) --> (N, 3)\n",
    "        x = x.view(-1, 3)\n",
    "        # (N, 5)\n",
    "        x = self.fc1(x)\n",
    "        # (N, 1)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'in_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-c033abcb95e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'in_channels'"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(90):\n",
    "    for iteration in range(5005):\n",
    "        \n",
    "        # grad update\n",
    "        inp \n",
    "        out = net(inp)\n",
    "\n",
    "        loss = LOSS(out, label)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader, DataSampler\n",
    "## DataAugmentation: crop, flip, rotate, color\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
